{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkTwH2o86O6V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45dff78"
      },
      "source": [
        "# Task\n",
        "Unzip `archive.zip`, load the main CSV dataset into a Pandas DataFrame, perform an initial exploration by showing the first 5 rows and checking for null values, and then generate a bar chart to visualize the balance of sentiment categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbf4730f"
      },
      "source": [
        "## Descompactar Arquivo\n",
        "\n",
        "### Subtask:\n",
        "Descompactar o arquivo 'archive.zip' para acessar os datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6cb724"
      },
      "source": [
        "## Análise de Sentimento com o Modelo Treinado\n",
        "\n",
        "### Subtask:\n",
        "Criar uma função `analisar_mercado(texto)` que utiliza o modelo FinBERT para prever o sentimento de uma manchete e testá-la com um exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92b3c550"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the `analisar_mercado` function, I will use the previously loaded `tokenizer` and `model`. The function will tokenize the input text, move the tensors to the GPU if available, get predictions from the model, apply softmax to convert logits to probabilities, and then map the predicted class index to a sentiment label (Negative or Positive) based on the `num_labels` configuration. Finally, I will test the function with the given headline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36e8e46c"
      },
      "source": [
        "import torch\n",
        "\n",
        "def analisar_mercado(texto):\n",
        "    \"\"\"\n",
        "    Analisa o sentimento de uma manchete de notícia usando o modelo FinBERT treinado.\n",
        "\n",
        "    Args:\n",
        "        texto (str): A manchete de notícia a ser analisada.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Uma tupla contendo o rótulo de sentimento (string) e a probabilidade (float).\n",
        "    \"\"\"\n",
        "    # Tokenizar a manchete\n",
        "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "    # Mover inputs para o dispositivo (GPU se disponível)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Fazer a previsão\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Obter logits e calcular probabilidades\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    predicted_class_id = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    # Mapear o ID da classe para o rótulo de sentimento\n",
        "    # Baseado em como mapeamos -1 para 0 e 1 para 1 no pré-processamento\n",
        "    sentiment_labels = {0: 'Negative', 1: 'Positive'}\n",
        "    sentiment_label = sentiment_labels.get(predicted_class_id, 'Unknown')\n",
        "    confidence = probabilities[0, predicted_class_id].item()\n",
        "\n",
        "    return sentiment_label, confidence\n",
        "\n",
        "# Testar a função com a manchete fornecida\n",
        "test_headline = \"Apple stock price plunged 5% after the new quarterly earnings report showed massive disappointment.\"\n",
        "sentiment, confidence = analisar_mercado(test_headline)\n",
        "\n",
        "print(f\"Manchete: '{test_headline}'\")\n",
        "print(f\"Sentimento Previsto: {sentiment}\")\n",
        "print(f\"Confiança do Modelo: {confidence:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcb19ec"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract the contents of 'archive.zip', I will execute the `!unzip archive.zip` command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f271865"
      },
      "source": [
        "!unzip archive.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc9bef6"
      },
      "source": [
        "## Carregar Dataset Principal\n",
        "\n",
        "### Subtask:\n",
        "Carregar o arquivo CSV principal descompactado em um DataFrame do Pandas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78875c3"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the main CSV dataset into a Pandas DataFrame, I will first import the pandas library and then use `pd.read_csv()` to load 'stock_data.csv' into a DataFrame named `df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efeafb01"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('stock_data.csv')\n",
        "print(\"Dataset loaded successfully.\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519edd25"
      },
      "source": [
        "## Exploração Inicial do DataFrame\n",
        "\n",
        "### Subtask:\n",
        "Exibir as 5 primeiras linhas do DataFrame e verificar a presença de valores nulos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c14e69c"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask of checking for null values, I will use the `.isnull().sum()` method on the DataFrame `df` to get a count of missing values per column. This will provide an overview of data completeness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2e8e1c"
      },
      "source": [
        "print(\"Checking for null values in the DataFrame:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa8dc68"
      },
      "source": [
        "## Análise de Balanceamento de Sentimento\n",
        "\n",
        "### Subtask:\n",
        "Gerar um gráfico de barras para visualizar a contagem de cada categoria de 'Sentiment' e analisar o balanceamento dos dados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "961ab28b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `archive.zip` file was successfully unzipped, and the `stock_data.csv` file was extracted.\n",
        "*   The `stock_data.csv` dataset was loaded into a Pandas DataFrame for further analysis.\n",
        "*   An initial check for null values confirmed that both the 'Text' and 'Sentiment' columns in the DataFrame contain no missing values.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The dataset is clean and ready for sentiment analysis without requiring handling of missing values in the key 'Text' and 'Sentiment' columns.\n",
        "*   The next crucial step is to generate a bar chart to visualize the distribution of sentiment categories to understand the dataset's balance, as this was the final part of the initial task setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa0d1b9e"
      },
      "source": [
        "# Task\n",
        "To proceed with the sentiment analysis, the next step is to verify the unique sentiment classes in the dataset, map them to a consistent integer format if necessary, and then display this mapping along with the distribution of each class to ensure proper preparation for model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97c382a2"
      },
      "source": [
        "## Verificação e Mapeamento de Classes de Sentimento\n",
        "\n",
        "### Subtask:\n",
        "Verificar os valores únicos da coluna 'Sentiment', mapeá-los para inteiros (0, 1 ou 0, 1, 2) conforme necessário e imprimir o mapeamento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738fea34"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify and map the 'Sentiment' column, I will first identify and print the unique values to understand the current sentiment categories. Then, I will create a mapping based on the instructions to convert '-1' to '0' and keep '1' as '1', and apply this mapping to the 'Sentiment' column. Finally, I will display the value counts of the updated 'Sentiment' column to confirm the new distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "187c038e"
      },
      "source": [
        "print(\"Unique values in 'Sentiment' before mapping:\", df['Sentiment'].unique())\n",
        "\n",
        "# Check unique values and apply mapping if necessary\n",
        "if set(df['Sentiment'].unique()) == {-1, 1}:\n",
        "    sentiment_mapping = {-1: 0, 1: 1}\n",
        "    df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
        "    print(\"Sentiment mapped: -1 to 0, 1 to 1\")\n",
        "    print(\"Mapping applied:\", sentiment_mapping)\n",
        "else:\n",
        "    print(\"No specific mapping applied as unique sentiment values are not exactly {-1, 1}.\")\n",
        "\n",
        "print(\"\\nSentiment class distribution after mapping:\")\n",
        "print(df['Sentiment'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2be0a3c"
      },
      "source": [
        "## Carregamento do Modelo e Tokenizer FinBERT\n",
        "\n",
        "### Subtask:\n",
        "Carregar o modelo 'ProsusAI/finbert' e seu tokenizer correspondente, configurando 'num_labels' com base no número de classes de sentimento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78759958"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the FinBERT model and tokenizer, I will import the necessary classes from the `transformers` library, determine the number of unique sentiment classes from the 'Sentiment' column, and then use these to load the pre-trained tokenizer and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a40daf7b"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Determine the number of unique sentiment classes\n",
        "num_labels = df['Sentiment'].nunique()\n",
        "print(f\"Number of sentiment classes: {num_labels}\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "print(\"FinBERT tokenizer loaded successfully.\")\n",
        "\n",
        "# Load the model, configuring num_labels\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", num_labels=num_labels)\n",
        "print(\"FinBERT model loaded successfully with custom num_labels.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8d92dec"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `RuntimeError` due to a size mismatch in the classification layer of the FinBERT model. The pre-trained FinBERT model expects 3 output labels (positive, negative, neutral), but the current task requires only 2 (mapped to 0 and 1). To resolve this, I need to instruct `from_pretrained` to ignore size mismatches in the classification head, allowing it to be reinitialized for the correct number of labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31d77175"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Determine the number of unique sentiment classes\n",
        "num_labels = df['Sentiment'].nunique()\n",
        "print(f\"Number of sentiment classes: {num_labels}\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "print(\"FinBERT tokenizer loaded successfully.\")\n",
        "\n",
        "# Load the model, configuring num_labels and ignoring mismatched sizes for the classifier head\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", num_labels=num_labels, ignore_mismatched_sizes=True)\n",
        "print(\"FinBERT model loaded successfully with custom num_labels and mismatched sizes ignored.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1c5333"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code successfully loaded the FinBERT model and tokenizer. However, the initial task included generating a bar chart to visualize the balance of sentiment categories, which has not yet been done. Visualizing the sentiment distribution is crucial for understanding the dataset's balance before proceeding with model training or evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ae3be6c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the value counts of the 'Sentiment' column\n",
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"viridis\")\n",
        "plt.title('Distribution of Sentiment Categories')\n",
        "plt.xlabel('Sentiment (0: Negative, 1: Positive)')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(ticks=[0, 1], labels=['Negative', 'Positive'])\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart showing sentiment distribution generated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c0a22b0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code generated a `FutureWarning` from `seaborn` regarding the `palette` argument without `hue`. To fix this and comply with future `seaborn` versions, I will explicitly set `hue` to the `x` variable and add `legend=False` as suggested by the warning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "574c9ed2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the value counts of the 'Sentiment' column\n",
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"viridis\", hue=sentiment_counts.index, legend=False)\n",
        "plt.title('Distribution of Sentiment Categories')\n",
        "plt.xlabel('Sentiment (0: Negative, 1: Positive)')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(ticks=[0, 1], labels=['Negative', 'Positive'])\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart showing sentiment distribution generated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14eb95c"
      },
      "source": [
        "## Tokenização dos Dados\n",
        "\n",
        "### Subtask:\n",
        "Aplicar o tokenizer do FinBERT aos dados textuais do DataFrame para prepará-los para o modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42ba010"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply the FinBERT tokenizer to the textual data, I will first import the `Dataset` class, create a Hugging Face Dataset from the DataFrame, define a tokenization function that truncates sequences, and then apply this function using the `.map()` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3af300c"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(df, split='train')\n",
        "print(\"Hugging Face Dataset created successfully.\")\n",
        "\n",
        "# Define the tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['Text'], truncation=True, max_length=512)\n",
        "\n",
        "# Apply the tokenization function to the dataset\n",
        "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True, remove_columns=['Text'])\n",
        "print(\"Dataset tokenized successfully. Example of tokenized input:\")\n",
        "print(tokenized_datasets[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64e3637b"
      },
      "source": [
        "## Divisão dos Dados em Treino e Teste\n",
        "\n",
        "### Subtask:\n",
        "Dividir os dados tokenizados em conjuntos de treino (80%) e teste (20%) usando `stratify` na coluna de 'Sentiment' para manter o balanceamento das classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6890c5"
      },
      "source": [
        "**Reasoning**:\n",
        "To split the tokenized dataset into training and testing sets while maintaining the sentiment class balance, I will use the built-in `train_test_split` method of the Hugging Face `Dataset` object, specifying a `test_size` of 0.2 and stratifying by the 'Sentiment' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a093807"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "splitted_datasets = tokenized_datasets.train_test_split(test_size=0.2, stratify_by_column='Sentiment')\n",
        "\n",
        "train_dataset = splitted_datasets['train']\n",
        "test_dataset = splitted_datasets['test']\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Data split into training and test sets successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc454cf3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'Sentiment' column in the Hugging Face Dataset was not of type `ClassLabel`, which is required for stratification. To fix this, I need to first cast the 'Sentiment' column to `ClassLabel` with the appropriate number of classes before performing the `train_test_split`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95949273"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import ClassLabel, Features, Value\n",
        "\n",
        "# Define the features with ClassLabel for 'Sentiment'\n",
        "features = Features({\n",
        "    'input_ids': Value(dtype='int32'),\n",
        "    'token_type_ids': Value(dtype='int32'),\n",
        "    'attention_mask': Value(dtype='int32'),\n",
        "    'Sentiment': ClassLabel(num_classes=num_labels, names=['Negative', 'Positive'])\n",
        "})\n",
        "\n",
        "# Cast the dataset to include the ClassLabel for 'Sentiment'\n",
        "tokenized_datasets = tokenized_datasets.cast(features)\n",
        "print(\"Sentiment column cast to ClassLabel type.\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "splitted_datasets = tokenized_datasets.train_test_split(test_size=0.2, stratify_by_column='Sentiment')\n",
        "\n",
        "train_dataset = splitted_datasets['train']\n",
        "test_dataset = splitted_datasets['test']\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Data split into training and test sets successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd517a28"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error 'TypeError: Couldn't cast array of type list<item: int32> to int32' occurred because the `input_ids`, `token_type_ids`, and `attention_mask` columns, which are sequences of integers, were incorrectly defined as single `Value(dtype='int32')` in the `Features` object. To fix this, these columns must be defined as `Sequence(Value(dtype='int32'))` to correctly represent arrays or lists of integers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "facd239d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import ClassLabel, Features, Value, Sequence\n",
        "\n",
        "# Define the features with ClassLabel for 'Sentiment' and Sequence for tokenized inputs\n",
        "features = Features({\n",
        "    'input_ids': Sequence(feature=Value(dtype='int32')),\n",
        "    'token_type_ids': Sequence(feature=Value(dtype='int32')),\n",
        "    'attention_mask': Sequence(feature=Value(dtype='int32')),\n",
        "    'Sentiment': ClassLabel(num_classes=num_labels, names=['Negative', 'Positive'])\n",
        "})\n",
        "\n",
        "# Cast the dataset to include the ClassLabel for 'Sentiment'\n",
        "tokenized_datasets = tokenized_datasets.cast(features)\n",
        "print(\"Sentiment column and tokenized inputs cast to correct types.\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "splitted_datasets = tokenized_datasets.train_test_split(test_size=0.2, stratify_by_column='Sentiment')\n",
        "\n",
        "train_dataset = splitted_datasets['train']\n",
        "test_dataset = splitted_datasets['test']\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Data split into training and test sets successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9da3f78"
      },
      "source": [
        "## Configuração do Treinador (Trainer)\n",
        "\n",
        "### Subtask:\n",
        "Configurar o objeto `Trainer` do Hugging Face Transformers com `num_train_epochs=3`, `per_device_train_batch_size=16`, e definir as métricas de acurácia e F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5ac34f"
      },
      "source": [
        "**Reasoning**:\n",
        "To configure the Hugging Face Trainer, I need to import `TrainingArguments` and `Trainer`, define a `compute_metrics` function using `accuracy_score` and `f1_score`, initialize `TrainingArguments` with the specified parameters, and then instantiate the `Trainer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4f2c070"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted') # Using weighted for potential class imbalance\n",
        "    return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
        "\n",
        "# Initialize TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16, # Added to match train batch size as per instructions\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset, # Use test_dataset for evaluation as per common practice\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer configured successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6844ec2e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `evaluation_strategy` is an outdated argument for `TrainingArguments`. It has been renamed to `eval_strategy` in newer versions of the `transformers` library. I will update the argument name to `eval_strategy` to resolve this `TypeError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c0be342"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted') # Using weighted for potential class imbalance\n",
        "    return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
        "\n",
        "# Initialize TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16, # Added to match train batch size as per instructions\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy='epoch', # Corrected from evaluation_strategy\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset, # Use test_dataset for evaluation as per common practice\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer configured successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380539b3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code generated a `FutureWarning` indicating that the `tokenizer` argument in `Trainer.__init__` is deprecated and should be replaced with `processing_class`. To comply with future versions of the `transformers` library and eliminate the warning, I will update the `Trainer` instantiation to use `processing_class=tokenizer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ff9405"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted') # Using weighted for potential class imbalance\n",
        "    return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
        "\n",
        "# Initialize TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16, # Added to match train batch size as per instructions\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy='epoch', # Corrected from evaluation_strategy\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset, # Use test_dataset for evaluation as per common practice\n",
        "    processing_class=tokenizer, # Corrected from tokenizer=tokenizer\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer configured successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "743a80a3"
      },
      "source": [
        "## Execução do Treinamento\n",
        "\n",
        "### Subtask:\n",
        "Iniciar o treinamento do modelo usando o `Trainer`, garantindo que a GPU (cuda) seja utilizada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efd109cc"
      },
      "source": [
        "**Reasoning**:\n",
        "To initiate the training process as per the subtask, I will first verify GPU availability and then call the `train()` method on the configured `trainer` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a91b1a3"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU (cuda) is available. Training will utilize GPU.\")\n",
        "else:\n",
        "    print(\"GPU (cuda) is not available. Training will run on CPU.\")\n",
        "\n",
        "# Start training the model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c4f0b90"
      },
      "source": [
        "# Task\n",
        "Ajustar a configuração de treinamento para desativar o registro no Weights & Biases (W&B) e, em seguida, re-executar o treinamento do modelo FinBERT para análise de sentimento. Por fim, forneça um resumo do processo de treinamento e das métricas de desempenho obtidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b507e0c"
      },
      "source": [
        "## Ajustar Configuração do Trainer\n",
        "\n",
        "### Subtask:\n",
        "Modificar a inicialização do `TrainingArguments` para incluir `report_to='none'`, desativando o registro no Weights & Biases (W&B) e prevenindo o prompt de login.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "569fcecb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous training attempt failed because of a Weights & Biases login prompt. To disable this, I need to add `report_to='none'` to the `TrainingArguments` initialization as specified in the subtask. I will regenerate the entire code block for configuring the Trainer to incorporate this change.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9adcea34"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted') # Using weighted for potential class imbalance\n",
        "    return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
        "\n",
        "# Initialize TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    report_to='none' # Added to disable Weights & Biases logging\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer configured successfully with W&B logging disabled.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f25d4071"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully reconfigured the Trainer to disable Weights & Biases logging. The next step, as per the main task, is to re-execute the training of the FinBERT model to ensure it runs without interruption.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09401373"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU (cuda) is available. Training will utilize GPU.\")\n",
        "else:\n",
        "    print(\"GPU (cuda) is not available. Training will run on CPU.\")\n",
        "\n",
        "# Start training the model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c935f96"
      },
      "source": [
        "**Reasoning**:\n",
        "The training failed because the model expected a 'labels' column for loss computation, but the dataset provided 'Sentiment'. I need to rename the 'Sentiment' column to 'labels' in both the training and test datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8d7359a"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Rename the 'Sentiment' column to 'labels' in both train and test datasets\n",
        "# The Trainer expects the labels column to be named 'labels' by default.\n",
        "train_dataset = train_dataset.rename_column(\"Sentiment\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"Sentiment\", \"labels\")\n",
        "\n",
        "print(\"Sentiment column renamed to labels in train and test datasets.\")\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU (cuda) is available. Training will utilize GPU.\")\n",
        "else:\n",
        "    print(\"GPU (cuda) is not available. Training will run on CPU.\")\n",
        "\n",
        "# Start training the model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39995f42"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous training attempt failed because the `trainer` object was initialized before the 'Sentiment' column in the datasets was renamed to 'labels'. To fix this, I need to re-instantiate the `Trainer` object using the `train_dataset` and `test_dataset` which now correctly have the 'labels' column for loss computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3be66fc2"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted') # Using weighted for potential class imbalance\n",
        "    return {\"accuracy\": accuracy, \"f1_score\": f1}\n",
        "\n",
        "# Initialize TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    report_to='none' # Added to disable Weights & Biases logging\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer re-configured successfully with W&B logging disabled and updated datasets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bb5407"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully re-configured the Trainer with the `labels` column and disabled W&B logging. Now, I need to re-initiate the training process to ensure the model trains correctly without errors, as specified by the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16d5ed85"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU (cuda) is available. Training will utilize GPU.\")\n",
        "else:\n",
        "    print(\"GPU (cuda) is not available. Training will run on CPU.\")\n",
        "\n",
        "# Start training the model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7832714d"
      },
      "source": [
        "**Reasoning**:\n",
        "The model training has been successfully completed. Now, I will evaluate the trained model on the test dataset to get the performance metrics and then summarize the training process and the obtained metrics, as per the main task instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "469334e2"
      },
      "source": [
        "print(\"Evaluating the trained model on the test dataset...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"Training process and evaluation metrics:\")\n",
        "print(f\"Epochs trained: {eval_results['epoch']}\")\n",
        "print(f\"Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Evaluation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Evaluation F1-Score (weighted): {eval_results['eval_f1_score']:.4f}\")\n",
        "print(\"\\nSummary of Training:\\n\")\n",
        "print(f\"Model training was successfully completed over {training_args.num_train_epochs} epochs.\")\n",
        "print(f\"The model was trained with a batch size of {training_args.per_device_train_batch_size} on GPU (if available).\")\n",
        "print(\"Weights & Biases logging was disabled to prevent interruptions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04d84ab0"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Fornecer um resumo do processo de treinamento do modelo FinBERT para análise de sentimento, incluindo as métricas de desempenho obtidas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38b29b6a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The FinBERT model training process for sentiment analysis involved adjusting the `TrainingArguments` to disable Weights & Biases (W&B) logging, resolving a dataset column mismatch, and then training the model. The final performance metrics obtained were an Evaluation Loss of 0.5413, an Evaluation Accuracy of 0.8179, and an Evaluation F1-Score (weighted) of 0.8169 after 3 epochs of training.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `report_to='none'` parameter was successfully integrated into `TrainingArguments`, disabling W&B logging and preventing prior interruptions.\n",
        "*   An initial `ValueError` was encountered because the `Trainer` expected a 'labels' column, while the dataset used 'Sentiment'; this was rectified by renaming the 'Sentiment' column to 'labels' in both training and test datasets.\n",
        "*   The `Trainer` required re-instantiation after the dataset column renaming for the changes to be recognized.\n",
        "*   After these adjustments, the FinBERT model was successfully trained for 3 epochs using GPU resources.\n",
        "*   The trained model achieved an Evaluation Loss of 0.5413, an Evaluation Accuracy of 0.8179, and an Evaluation F1-Score (weighted) of 0.8169 on the test dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Ensuring dataset column names (e.g., 'labels') align with the expectations of the `Trainer` is crucial for successful model training.\n",
        "*   To potentially enhance model performance, further hyperparameter tuning (e.g., adjusting `num_train_epochs`, learning rate, or `weight_decay`) could be explored.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e06cc13"
      },
      "source": [
        "# Task\n",
        "## Gerar Previsões no Conjunto de Teste\n",
        "\n",
        "### Subtask:\n",
        "Utilizar o `trainer` treinado para gerar previsões (logits) no `test_dataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0475d5f"
      },
      "source": [
        "## Gerar Previsões no Conjunto de Teste\n",
        "\n",
        "### Subtask:\n",
        "Utilizar o `trainer` treinado para gerar previsões (logits) no `test_dataset`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cffaaa52"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate predictions on the `test_dataset` using the trained `trainer`, I will use the `trainer.predict()` method, which will output the logits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fd2e3b4"
      },
      "source": [
        "print(\"Generating predictions on the test dataset...\")\n",
        "predictions = trainer.predict(test_dataset)\n",
        "print(\"Predictions generated successfully.\")\n",
        "# The 'predictions' object will contain logits, and potentially labels and metrics if available."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e971dd23"
      },
      "source": [
        "## Gerar e Imprimir Relatório de Classificação\n",
        "\n",
        "### Subtask:\n",
        "Extrair os rótulos verdadeiros e as previsões do modelo e usar `sklearn.metrics.classification_report` para imprimir a precisão, recall e F1-score para cada classe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6eb16f7"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate and print the classification report, I will import `classification_report` from `sklearn.metrics`, extract the true labels from `predictions.label_ids` and predicted labels from `predictions.predictions` using `np.argmax`, and then print the report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "488ffc1f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Extract true labels\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "# Extract predicted labels from logits\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels, target_names=['Negative', 'Positive']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e8ebeda"
      },
      "source": [
        "## Gerar e Plotar Matriz de Confusão\n",
        "\n",
        "### Subtask:\n",
        "Calcular a matriz de confusão usando `sklearn.metrics.confusion_matrix` e plotá-la visualmente com `seaborn.heatmap` para identificar onde o modelo cometeu erros de classificação.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6bf960"
      },
      "source": [
        "**Reasoning**:\n",
        "To calculate and plot the confusion matrix, I will import the necessary libraries (`confusion_matrix`, `matplotlib.pyplot`, `seaborn`), compute the confusion matrix using the `true_labels` and `predicted_labels`, and then visualize it as a heatmap with appropriate labels and a title.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "068fd897"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix calculated and plotted successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4891598b"
      },
      "source": [
        "## Função para Aplicar o Modelo a um Novo Conjunto de Dados\n",
        "\n",
        "### Subtask:\n",
        "Criar uma função que aceite o caminho de um arquivo CSV, aplique o modelo FinBERT treinado para prever o sentimento de cada manchete, adicione as colunas 'Previsão_Sentimento' e 'Confiança' ao DataFrame e salve o resultado em 'previsoes_finais.csv'. Demonstrar o uso da função com um DataFrame de teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d5ef4e9"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a Python function `apply_finbert_to_csv` that takes the input CSV path, reads it into a pandas DataFrame, and then iterates through the 'Text' column. For each text, it will call the already defined `analisar_mercado` function to get the predicted sentiment and confidence. These results will be stored in new columns 'Previsão_Sentimento' and 'Confiança'. Finally, the function will save the modified DataFrame to 'previsoes_finais.csv'. To demonstrate, I will create a sample DataFrame, save it as a temporary CSV, and then call the new function with this CSV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2330395c"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def apply_finbert_to_csv(input_csv_path, output_csv_path='previsoes_finais.csv'):\n",
        "    \"\"\"\n",
        "    Aplica o modelo FinBERT a um conjunto de dados CSV, prevê o sentimento e a confiança,\n",
        "    e salva o resultado em um novo arquivo CSV.\n",
        "\n",
        "    Args:\n",
        "        input_csv_path (str): Caminho para o arquivo CSV de entrada com uma coluna 'Text'.\n",
        "        output_csv_path (str): Caminho para salvar o arquivo CSV de saída com as previsões.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: O DataFrame com as previsões adicionadas.\n",
        "    \"\"\"\n",
        "    print(f\"Lendo o arquivo CSV de entrada: {input_csv_path}\")\n",
        "    df_new = pd.read_csv(input_csv_path)\n",
        "\n",
        "    # Certifica-se de que a coluna 'Text' existe\n",
        "    if 'Text' not in df_new.columns:\n",
        "        raise ValueError(\"O arquivo CSV de entrada deve conter uma coluna 'Text'.\")\n",
        "\n",
        "    # Inicializa as listas para armazenar as previsões\n",
        "    predicted_sentiments = []\n",
        "    confidences = []\n",
        "\n",
        "    print(\"Aplicando o modelo FinBERT para prever sentimentos...\")\n",
        "    for index, row in df_new.iterrows():\n",
        "        sentiment, confidence = analisar_mercado(row['Text'])\n",
        "        predicted_sentiments.append(sentiment)\n",
        "        confidences.append(confidence)\n",
        "\n",
        "    # Adiciona as novas colunas ao DataFrame\n",
        "    df_new['Previsão_Sentimento'] = predicted_sentiments\n",
        "    df_new['Confiança'] = confidences\n",
        "\n",
        "    # Salva o DataFrame resultante em um novo arquivo CSV\n",
        "    df_new.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Resultados salvos em: {output_csv_path}\")\n",
        "\n",
        "    return df_new\n",
        "\n",
        "# --- Exemplo de Uso da Função ---\n",
        "\n",
        "# 1. Criar um DataFrame de teste com novas manchetes\n",
        "sample_data = {\n",
        "    'Text': [\n",
        "        \"Tesla's stock price soared by 10% after announcing record-breaking deliveries.\",\n",
        "        \"The company faced unexpected challenges leading to a significant drop in revenue.\",\n",
        "        \"Analysts are neutral on the market outlook, awaiting further economic data.\",\n",
        "        \"New product launch generated immense positive buzz and pre-orders.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_sample_test = pd.DataFrame(sample_data)\n",
        "\n",
        "# Salvar o DataFrame de teste em um CSV temporário para simular o arquivo de entrada\n",
        "sample_input_csv = 'sample_news_headlines.csv'\n",
        "df_sample_test.to_csv(sample_input_csv, index=False)\n",
        "print(f\"Arquivo de teste '{sample_input_csv}' criado com sucesso.\")\n",
        "\n",
        "# 2. Chamar a função para aplicar o modelo\n",
        "final_predictions_df = apply_finbert_to_csv(sample_input_csv)\n",
        "\n",
        "# 3. Exibir o DataFrame resultante\n",
        "print(\"\\nDataFrame final com previsões:\")\n",
        "display(final_predictions_df.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d6f192d"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Fornecer um resumo da avaliação do modelo, incluindo as métricas do relatório de classificação e as observações da matriz de confusão.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad8225e9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The model evaluation indicates an overall accuracy of 0.82. The 'Positive' class shows stronger performance with a precision of 0.85, recall of 0.87, and F1-score of 0.86. The 'Negative' class has slightly lower performance metrics, with a precision of 0.76, recall of 0.72, and F1-score of 0.74. The confusion matrix visually confirms these findings, showing that the model correctly classified more 'Positive' instances than 'Negative' instances.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The model achieved an overall accuracy of 0.82 on the test dataset.\n",
        "*   For the 'Negative' class (support 421): Precision is 0.76, Recall is 0.72, and F1-score is 0.74.\n",
        "*   For the 'Positive' class (support 738): Precision is 0.85, Recall is 0.87, and F1-score is 0.86.\n",
        "*   The confusion matrix revealed that out of 1159 instances, the model correctly identified 303 'Negative' instances (True Negatives) and 642 'Positive' instances (True Positives).\n",
        "*   The model misclassified 118 'Negative' instances as 'Positive' (False Positives) and 96 'Positive' instances as 'Negative' (False Negatives).\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Given the imbalance in recall (0.72 for 'Negative' vs. 0.87 for 'Positive'), investigate methods to improve the model's ability to identify 'Negative' instances, such as adjusting class weights during training or exploring different loss functions.\n",
        "*   Analyze the False Positives and False Negatives specifically to understand common characteristics of misclassified samples, which could inform feature engineering or data augmentation strategies.\n"
      ]
    }
  ]
}